{
 "cells": [
  {
   "metadata": {
    "id": "71845e863f75ae52"
   },
   "cell_type": "markdown",
   "source": [
    "# step1-offline-process"
   ],
   "id": "71845e863f75ae52"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e784003a1b39cbc",
    "outputId": "8769b4d4-94f4-4c0c-96f7-492f5edf37c6"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "成功将 1 个文本块存入向量数据库\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from zhipuai import ZhipuAI\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "# 1. 加载文档\n",
    "loader = TextLoader(\"knowledge_base.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. 文本切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # 每个文本块的大小\n",
    "    chunk_overlap=50,  # 文本块之间的重叠部分\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. 向量化并存储\n",
    "embeddings = ZhipuAIEmbeddings(\n",
    "    api_key=\"\", # 在 bigmodel.cn 控制台获取\n",
    "    model=\"embedding-2\" # 智谱的文本向量模型\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\",  # 持久化存储路径\n",
    ")\n",
    "\n",
    "print(f\"成功将 {len(splits)} 个文本块存入向量数据库\")\n"
   ],
   "id": "e784003a1b39cbc"
  },
  {
   "cell_type": "code",
   "source": [
    "splits"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QopVkoB6qt-q",
    "outputId": "9e2c530e-8f5c-41f8-fd6e-ae7d85d970f7"
   },
   "id": "QopVkoB6qt-q",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'knowledge_base.txt'}, page_content='RAG 的的本质是：增强 System Prompt 的上下文，从而生成更符合实际要求的回复。')]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "metadata": {
    "id": "65f606215110e3a6"
   },
   "cell_type": "markdown",
   "source": [
    "# step2-online-rag"
   ],
   "id": "65f606215110e3a6"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbd04a66c155e14f",
    "outputId": "0af22e31-ea1d-4de5-be0a-ab894240afe4"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "最终的 Prompt 内容：\n",
      "你是一个专业的问答助手。请根据以下参考文档回答用户的问题。\n",
      "如果参考文档中没有相关信息，请诚实地说不知道，不要编造答案。\n",
      "\n",
      "参考文档：\n",
      "RAG 的的本质是：增强 System Prompt 的上下文，从而生成更符合实际要求的回复。\n",
      "\n",
      "用户问题：什么是RAG？\n",
      "\n",
      "回答：\n",
      "\n",
      "问题: 什么是RAG？\n",
      "回答: RAG的本质是增强System Prompt的上下文，从而生成更符合实际要求的回复。简单来说，RAG是一种技术，它通过提升系统提示的上下文信息，使得生成的回答更加准确和符合用户的需求。\n",
      "\n",
      "参考文档数量: 1\n"
     ]
    }
   ],
   "execution_count": 22,
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 1. 加载已有的向量数据库\n",
    "embeddings = ZhipuAIEmbeddings(\n",
    "    api_key=\"\", # 在 bigmodel.cn 控制台获取\n",
    "    model=\"embedding-2\" # 智谱的文本向量模型\n",
    ")\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "\n",
    "# 2. 用户提问\n",
    "query = \"什么是RAG？\"\n",
    "\n",
    "# 3. 检索相关文档（返回最相关的 3 个）\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "# 4. 将检索到的文档内容拼接成上下文\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 5. 构建 Prompt 模板\n",
    "prompt_template = \"\"\"\n",
    "你是一个专业的问答助手。请根据以下参考文档回答用户的问题。\n",
    "如果参考文档中没有相关信息，请诚实地说不知道，不要编造答案。\n",
    "\n",
    "参考文档：\n",
    "{context}\n",
    "\n",
    "用户问题：{question}\n",
    "\n",
    "回答：\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n",
    "\n",
    "# 6. 创建 LLM 并生成回答\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4\",\n",
    "    model_name=\"glm-4\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(context=context, question=query)\n",
    "\n",
    "print(f\"最终的 Prompt 内容：{final_prompt}\")\n",
    "\n",
    "messages = [HumanMessage(content=final_prompt)]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 7. 输出结果\n",
    "print(f\"问题: {query}\")\n",
    "print(f\"回答: {response.content}\")\n",
    "print(f\"\\n参考文档数量: {len(docs)}\")\n"
   ],
   "id": "cbd04a66c155e14f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
